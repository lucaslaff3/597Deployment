{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4553ea9-d47e-4aaa-9424-df873f3570b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import joblib\n",
    "import json\n",
    "import pathlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read data\n",
    "data_filepath = pathlib.Path('train.csv')\n",
    "data = pd.read_csv(data_filepath)\n",
    "\n",
    "# Create \"dummy\" columns for categorical data\n",
    "dummy_column_mapper = {}\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        temp = pd.get_dummies(data[col], prefix=col, drop_first=True)\n",
    "        data = data.drop(columns=[col])\n",
    "        data[temp.columns] = temp\n",
    "        dummy_column_mapper[col] = temp.columns.tolist()\n",
    "\n",
    "# Save mapper for dummy columns\n",
    "with open('dummy_column_mapper.json', 'w') as fout:\n",
    "    json.dump(dummy_column_mapper, fout)\n",
    "\n",
    "# Prepare data for model training\n",
    "target = 'Exited'\n",
    "features = [col for col in data.columns if col != target]\n",
    "binary_columns = [col for col in features if sorted(data[col].unique().tolist()) == [0, 1]]\n",
    "\n",
    "X = data[features].copy()\n",
    "y = data[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.25, \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Save column order of training data\n",
    "with open('col_order.json', 'w') as fout:\n",
    "    json.dump(X_train.columns.tolist(), fout)\n",
    "\n",
    "# Fit scaler\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "\n",
    "# Save scaling information\n",
    "scaler_filepath = pathlib.Path('scaler_info.json')\n",
    "\n",
    "scaler_dict  = {}\n",
    "for feature, mean, scale in zip(features, scaler.mean_, scaler.scale_):\n",
    "    if feature in binary_columns:\n",
    "        scaler_dict[feature] = {\n",
    "            'mean': 0,\n",
    "            'std': 1,\n",
    "        }\n",
    "    else:\n",
    "        scaler_dict[feature] = {\n",
    "            'mean': mean,\n",
    "            'std': scale,\n",
    "        }\n",
    "        \n",
    "with open(scaler_filepath, 'w') as fout:\n",
    "    json.dump(scaler_dict, fout)\n",
    "    \n",
    "# Scale data\n",
    "for col, col_params in scaler_dict.items():\n",
    "    X_train.loc[:, col] = (X_train.loc[:, col] - col_params['mean'])/col_params['std']\n",
    "    X_test.loc[:, col] = (X_test.loc[:, col] - col_params['mean'])/col_params['std']\n",
    "\n",
    "# # Fit random forest model\n",
    "# params = {\n",
    "#     'criterion': ['gini', 'entropy'], \n",
    "#     'max_depth': [5, 10, 15], \n",
    "#     'n_estimators': [100, 300, 500], \n",
    "# }\n",
    "\n",
    "# clf = GridSearchCV(RandomForestClassifier(random_state=0), params, error_score=0)\n",
    "# search = clf.fit(X_train, y_train)\n",
    "# best_params = search.best_params_ \n",
    "\n",
    "# clf = RandomForestClassifier(random_state=0, **best_params)\n",
    "# clf = clf.fit(X_train.values, y_train.values) \n",
    "\n",
    "# # Save model\n",
    "# joblib.dump(clf, 'rf_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9afb62d2-4c64-4470-9786-51fa7385e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores =  cross_val_score(logistic,X_select,y,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "202a8adf-d52c-4bd2-a0f5-f4792a3a04af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8680304853590052"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit random forest model\n",
    "params = {\n",
    "    'criterion': ['gini', 'entropy'], \n",
    "    'max_depth': [5, 10, 15], \n",
    "    'n_estimators': [100, 300, 500], \n",
    "}\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(random_state=0), params, error_score=0)\n",
    "search = clf.fit(X_train, y_train)\n",
    "best_params = search.best_params_ \n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, **best_params)\n",
    "clf = clf.fit(X_train.values, y_train.values) \n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34a7d75e-90ac-46e5-878f-887b6f684239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8126754913758524"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit logistic regression model\n",
    "params = {\n",
    "    'C': [1, 1.5, 2, 2.5, 3], \n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n",
    "    'max_iter': [75, 100, 125],\n",
    "    'intercept_scaling': [.5, 1, 1.5, 2]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(random_state=0), params, error_score=0)\n",
    "search = clf.fit(X_train, y_train)\n",
    "best_params = search.best_params_ \n",
    "\n",
    "clf = LogisticRegression(random_state=0, **best_params)\n",
    "clf = clf.fit(X_train.values, y_train.values) \n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9607ae00-6034-49fc-a728-6080bb56ac64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8576012835940634"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "\n",
    "params = {\n",
    "    'n_estimators': [10, 100, 200],\n",
    "    'learning_rate': [0.25, 0.5, 1.0, 2.0],\n",
    "}    \n",
    "\n",
    "clf = GridSearchCV(AdaBoostClassifier(random_state=0), params, error_score=0)\n",
    "search = clf.fit(X_train, y_train)\n",
    "best_params = search.best_params_ \n",
    "\n",
    "clf = AdaBoostClassifier(random_state=0, **best_params) \n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4bb64d-8db3-4f67-ba48-3f2077d8d314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8547934215804252"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit decision tree model\n",
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [2, 5, 10, 15, 20], \n",
    "}\n",
    "\n",
    "clf = GridSearchCV(DecisionTreeClassifier(random_state=0), params, error_score=0)\n",
    "search = clf.fit(X_train, y_train)\n",
    "best_params = search.best_params_ \n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0, **best_params)\n",
    "clf = clf.fit(X_train.values, y_train.values) \n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859daf62-7aa3-4fac-850a-f26fd34e24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "params = {\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'hidden_layer_sizes': [3, 5, 7],\n",
    "    'learning_rate': ['contant', 'adaptive'],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(MLPClassifier(random_state=0), params, error_score=0)\n",
    "search = clf.fit(X_train, y_train)\n",
    "best_params = search.best_params_ \n",
    "\n",
    "clf = MLPClassifier(random_state=0, **best_params)\n",
    "clf = clf.fit(X_train.values, y_train.values) \n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2394dd93-f6c3-4850-b86d-b7b588c5590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Random Forest Classifier\n",
      "Fitting AdaBoost Classifier\n",
      "Fitting Neural Network Classifier\n",
      "Fitting Voting Classifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8688327316486161"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "print(f'Fitting Random Forest Classifier')\n",
    "params = {\n",
    "    'criterion': ['gini', 'entropy'], \n",
    "    'max_depth': [5, 10, 15], \n",
    "    'n_estimators': [400, 500, 600],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(random_state=0), params, error_score=0)\n",
    "search = clf.fit(X_train, y_train)\n",
    "best_params = search.best_params_ \n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0, **best_params)\n",
    "rf_clf = rf_clf.fit(X_train, y_train) \n",
    "\n",
    "\n",
    "print(f'Fitting AdaBoost Classifier')\n",
    "params = {\n",
    "    'n_estimators': [10, 100, 200],\n",
    "    'learning_rate': [0.5, 1.0, 1.5, 2.0],\n",
    "}    \n",
    "\n",
    "clf = GridSearchCV(AdaBoostClassifier(random_state=0), params, error_score=0)\n",
    "search = clf.fit(X_train, y_train)\n",
    "best_params = search.best_params_ \n",
    "\n",
    "ada_clf = AdaBoostClassifier(random_state=0, **best_params) \n",
    "ada_clf = ada_clf.fit(X_train, y_train) \n",
    "\n",
    "print(f'Fitting Neural Network Classifier')\n",
    "params = {\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'hidden_layer_sizes': [6, 7, 8],\n",
    "    'learning_rate': ['contant', 'adaptive'],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(MLPClassifier(random_state=0), params, error_score=0)\n",
    "search = clf.fit(X_train, y_train)\n",
    "best_params = search.best_params_ \n",
    "\n",
    "nn_clf = MLPClassifier(random_state=0, **best_params)\n",
    "nn_clf = clf.fit(X_train.values, y_train.values) \n",
    "\n",
    "print(f'Fitting Voting Classifier')\n",
    "params = {\n",
    "    'voting': ['hard', 'soft'],\n",
    "    'weights': [[0.34, 0.33, 0.33], [0.4, 0.3, 0.3], [0.3, 0.4, 0.3], [0.3, 0.3, 0.4], [0.4, 0.4, 0.2], [0.4, 0.2, 0.4], [0.2, 0.4, 0.4], [0.35, 0.35, 0.3]]\n",
    "}    \n",
    "\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('ada', ada_clf), ('nn', nn_clf)], \n",
    ")\n",
    "\n",
    "clf = GridSearchCV(eclf, params, error_score=0)\n",
    "search = clf.fit(X_train, y_train)\n",
    "best_params = search.best_params_ \n",
    "\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('ada', ada_clf), ('nn', nn_clf)],\n",
    "    **best_params,\n",
    ")\n",
    "\n",
    "eclf = eclf.fit(X_train, y_train)\n",
    "eclf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59452436-603e-4d0e-b1e0-9e22b075e238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'voting': 'hard', 'weights': [0.3, 0.33, 0.17, 0.2]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ea46d9-3d95-4eb4-ae90-d5d024345031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_model.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(eclf, 'rf_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "244dcdc4-6083-4198-ace3-6034f62aea71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CreditScore': 667,\n",
       " 'Geography': 'Germany',\n",
       " 'Gender': 'Male',\n",
       " 'Age': 55,\n",
       " 'Tenure': 9,\n",
       " 'Balance': 154393.43,\n",
       " 'NumOfProducts': 1,\n",
       " 'HasCrCard': 1,\n",
       " 'IsActiveMember': 1,\n",
       " 'EstimatedSalary': 137674.96}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = pd.read_csv(data_filepath)\n",
    "features = [col for col in new.columns if col != target]\n",
    "\n",
    "raw_payload = new[features].loc[15].to_dict()\n",
    "raw_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a83b8b46-7dc2-4dfc-b1d7-d4e3c0f000bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = {}\n",
    "for i in range(0,20):\n",
    "    features = [col for col in new.columns if col != target]\n",
    "\n",
    "    raw_payload = new[features].loc[i].to_dict()\n",
    "\n",
    "    with open('dummy_column_mapper.json') as fin:\n",
    "        dummy_column_mapper = json.load(fin)\n",
    "\n",
    "    with open('scaler_info.json') as fin:\n",
    "        scaler_info = json.load(fin)\n",
    "\n",
    "    with open('col_order.json') as fin:\n",
    "        col_order = json.load(fin)\n",
    "\n",
    "    payload = dict(raw_payload)\n",
    "    for column, dummy_columns in dummy_column_mapper.items():\n",
    "        for dummy_column in dummy_columns:\n",
    "            payload[dummy_column] = 0\n",
    "        if column in payload:\n",
    "            column_val = payload.pop(column)\n",
    "            target_column = f'{column}_{column_val}'\n",
    "            payload[target_column] = 1\n",
    "\n",
    "    for key, scaler_params in scaler_info.items():\n",
    "        if key in payload:\n",
    "            payload[key] = (payload[key] - scaler_params['mean'])/scaler_params['std']\n",
    "        else:\n",
    "            payload[key] = scaler_params['mean']\n",
    "\n",
    "    ordered_payload = {}\n",
    "    for col in col_order:\n",
    "        ordered_payload[col] = payload[col]\n",
    "\n",
    "    outcome[i+1] = int(clf.predict(np.array(list(ordered_payload.values())).reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df393bab-afad-4cf6-bc33-29189e3ebae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 2: 0,\n",
       " 3: 0,\n",
       " 4: 0,\n",
       " 5: 0,\n",
       " 6: 1,\n",
       " 7: 0,\n",
       " 8: 0,\n",
       " 9: 0,\n",
       " 10: 1,\n",
       " 11: 0,\n",
       " 12: 0,\n",
       " 13: 0,\n",
       " 14: 0,\n",
       " 15: 1,\n",
       " 16: 0,\n",
       " 17: 0,\n",
       " 18: 0,\n",
       " 19: 0,\n",
       " 20: 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed82e7b3-1403-4a83-8654-838b72d96388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2e8e8-7c3c-4fcd-ac70-89f49a60a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_endpoint = 'http://597deployment.azurewebsites.net/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8884c2-abb7-4da5-b1fe-38ce8a4a21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(base_endpoint)\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba1e2cc-8c69-42d8-a614-8b9d731280a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_endpoint = 'http://597deployment.azurewebsites.net/predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f7020-e4f9-4c08-936c-af882b8a8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(predict_endpoint, json=raw_payload)\n",
    "int(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fdb84-40aa-4634-9205-5922403786aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(data_filepath)\n",
    "\n",
    "for ckey in new_data.index.tolist()[:100]:\n",
    "    raw_payload = new_data.loc[ckey].to_dict()\n",
    "    target = raw_payload.pop('HeartDisease')\n",
    "    \n",
    "    r = requests.post(predict_endpoint, json=raw_payload)\n",
    "    prediction = int(r.text)\n",
    "    if prediction == target:\n",
    "        correct_statement = 'Correct!!!'\n",
    "    else:\n",
    "        correct_statement = ''\n",
    "\n",
    "    print(f'Instance {ckey}: actual->{target}, prediction->{prediction}. {correct_statement}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abcaa7c-2f77-4669-8a46-0e88142efd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "export FLASK_ENV=development"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
